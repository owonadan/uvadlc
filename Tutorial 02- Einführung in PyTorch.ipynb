{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: IntroducEinführung in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Willkommen zu unserem PyTorch-Tutorial für den Deep-Learning-Kurs 2023 an der Universität Amsterdam! Dieses Notebook bietet eine kurze Einführung in die Grundlagen von PyTorch und bereitet dich darauf vor, eigene neuronale Netze zu erstellen. PyTorch ist ein Open-Source-Framework für maschinelles Lernen, das dir flexibles Erstellen und effizientes Optimieren neuronaler Netze ermöglicht. PyTorch ist allerdings nicht das einzige Framework dieser Art. Alternativen sind [TensorFlow](https://www.tensorflow.org/), [JAX](https://github.com/google/jax#quickstart-colab-in-the-cloud) und [Caffe](http://caffe.berkeleyvision.org/). Wir haben uns an der Universität Amsterdam für PyTorch entschieden, da es gut etabliert ist, eine große Entwickler-Community hat (ursprünglich von Facebook entwickelt), sehr flexibel ist und insbesondere in der Forschung eingesetzt wird. Viele aktuelle wissenschaftliche Veröffentlichungen stellen ihren Code in PyTorch bereit, daher ist es von Vorteil, damit vertraut zu sein. TensorFlow (entwickelt von Google) ist hingegen primär als produktionsreife Deep-Learning-Bibliothek bekannt. Wenn du ein Machine-Learning-Framework bereits gut beherrschst, ist es leicht, ein anderes zu erlernen, da viele auf den gleichen Konzepten und Ideen basieren. Zum Beispiel wurde TensorFlow in Version 2 stark von den beliebtesten Features von PyTorch inspiriert, was die Frameworks einander noch ähnlicher gemacht hat. Falls du bereits mit PyTorch vertraut bist und eigene neuronale Netzwerk-Projekte erstellt hast, kannst du dieses Notebook gerne nur überfliegen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natürlich sind wir nicht die Ersten, die ein PyTorch-Tutorial erstellen. Es gibt viele großartige Tutorials im Internet, darunter der [\"60-min blitz\"](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) auf der offiziellen [PyTorch-Webseite](https://pytorch.org/tutorials/). Dennoch haben wir uns entschieden, ein eigenes Tutorial zu gestalten. Dieses soll dir die Grundlagen vermitteln, die du speziell für unsere praktischen Übungen benötigst, und dir trotzdem ein Verständnis der Funktionsweise von PyTorch geben. Im Laufe der nächsten Wochen werden wir auch weiterhin neue PyTorch-Features in dieser Reihe von Jupyter-Notebook-Tutorials rund um Deep Learning erkunden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir werden einen Satz von Standard-Bibliotheken verwenden, die häufig in Machine-Learning-Projekten genutzt werden. Wenn du dieses Notebook auf Google Colab ausführst, sollten alle Bibliotheken vorinstalliert sein. Falls du das Notebook lokal ausführst, stelle sicher, dass du unsere `d12023`-Umgebung installiert und aktiviert hast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nj/yxgl90t15y58nn503xvrtr4w0000gn/T/ipykernel_87459/47578708.py:11: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('svg', 'pdf') # For export\n"
     ]
    }
   ],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgba\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "## Progress bar\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Die Grundlagen von PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir beginnen mit einem Überblick über die grundlegenden Konzepte von PyTorch. Als Voraussetzung empfehlen wir, mit dem `numpy`-Paket vertraut zu sein, da die meisten Machine-Learning-Frameworks auf sehr ähnlichen Konzepten basieren. Falls du numpy noch nicht kennst, keine Sorge: hier findest du ein [Tutorial](https://numpy.org/devdocs/user/quickstart.html), um dich einzuarbeiten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los geht's! Beginnen wir damit, PyTorch zu importieren. Das Paket heißt `torch`, angelehnt an das ursprüngliche Framework Torch. Als ersten Schritt können wir die Version überprüfen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch 2.2.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Using torch\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zum Zeitpunkt der Erstellung dieses Tutorials (Ende Oktober 2023) ist die aktuelle stabile Version 2.1. Du solltest also als Ausgabe `Using torch 2.1.0` oder `Using torch 2.0.0` sehen, möglicherweise ergänzt durch die Angabe der CUDAVersion auf Colab. Falls du die `d12023`-Umgebung verwendest, solltest du `Using torch 2.1.0` sehen. Generell empfehlen wir, die PyTorch-Version stets auf dem aktuellsten Stand zu halten. Solltest du eine Versionsnummer unter 2.0 sehen, stelle sicher, dass du die richtige Umgebung installiert hast oder frage deine TAs. Im Fall, dass PyTorch 2.2 oder neuer während des Kurses veröffentlicht wird, brauchst du dir keine Sorgen zu machen. Die Schnittstelle zwischen PyTorch-Versionen ändert sich nicht drastisch, daher sollte der Code auch mit neueren Versionen lauffähig sein."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie in jedem Machine-Learning-Framework bietet PyTorch Funktionen für zufällige Vorgänge, wie die Erzeugung von Zufallszahlen. Es ist jedoch empfehlenswert, deinen Code so zu gestalten, dass Ergebnisse mit denselben Zufallszahlen reproduzierbar sind. Dies ermöglicht es dir, Fehler aufzuspüren und sicherzustellen, dass Änderungen deinerseits (und nicht zufällige Fluktuationen) die Ursache für veränderte Ergebnisse sind. Deshalb setzen wir nachfolgend einen \"Seed\" (Startwert). Der Seed ist der Ausgangspunkt für den Zufallszahlengenerator. Durch Setzen eines festen Seeds stellst du sicher, dass die Sequenz der erzeugten Zufallszahlen immer gleich ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x132033b70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42) # hier setzen wir den seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensoren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensoren sind das PyTorch-Äquivalent zu NumPy-Arrays, bieten aber zusätzlich Unterstützung für GPU-Beschleunigung (dazu später mehr). Der Name \"Tensor\" ist eine Verallgemeinerung von Konzepten, die du bereits kennst. Ein Vektor ist zum Beispiel ein 1D-Tensor und eine Matrix ein 2DTensor. Bei der Arbeit mit neuronalen Netzen verwenden wir Tensoren unterschiedlicher Formen und Dimensionen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die meisten gängigen Funktionen, die du von NumPy kennst, können auch auf Tensoren angewendet werden. Da NumPy-Arrays und Tensoren sich stark ähneln, können wir die meisten Tensoren in NumPy-Arrays (und umgekehrt) umwandeln. Dies ist jedoch in der Regel nicht oft notwendig."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fangen wir damit an, uns verschiedene Möglichkeiten zur Erstellung eines Tensors anzusehen. Es gibt viele Optionen, die einfachste ist der Aufruf von `torch.Tensor` mit der gewünschten Form als Eingabeparameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(2, 3, 4)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `torch.Tensor` weist den gewünschten Speicher für den Tensor zu, verwendet aber bereits vorhandene Speicherinhalte ggf. weiter. Um dem Tensor direkt bei der Initialisierung Werte zuzuweisen, gibt es verschiedene Möglichkeiten, darunter:\n",
    "- `torch.zeros`: Erzeugt einen Tensor gefüllt mit Nullen.\n",
    "- `torch.ones`: Erzeugt einen Tensor gefüllt mit Einsen.\n",
    "- `torch.rand`: Erzeugt einen Tensor mit Zufallswerten, die gleichmäßig zwischen 0 und 1 verteilt sind.\n",
    "- `torch.randn`: Erzeugt einen Tensor mit Zufallswerten, die einer Normalverteilung mit Mittelwert 0 und Varianz 1 folgen.\n",
    "- `torch.arange`: Erzeugt einen Tensor mit den Werten $N, N+1, N+2, \\ldots, M$\n",
    "- `torch.Tensor` (input list): Erzeugt einen Tensor aus einer von dir bereitgestellten Liste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "# Tensor aus (verschachtelter) Liste erzeugen\n",
    "x = torch.Tensor([[1, 2], [3, 4]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "         [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "         [0.9408, 0.1332, 0.9346, 0.5936]],\n",
      "\n",
      "        [[0.8694, 0.5677, 0.7411, 0.4294],\n",
      "         [0.8854, 0.5739, 0.2666, 0.6274],\n",
      "         [0.2696, 0.4414, 0.2969, 0.8317]]])\n"
     ]
    }
   ],
   "source": [
    "# Erstelle einen Tensor mit Zufallswerten zwischen 0 und 1 mit der Form [2, 3, 4]\n",
    "x = torch.rand(2, 3, 4)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du kannst die Form (Shape) eines Tensors wie in NumPy ermitteln (`x.shape`) oder alternativ mit der `.size`-Methode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape torch.Size([2, 3, 4])\n",
      "Size torch.Size([2, 3, 4])\n",
      "Size 2 3 4\n"
     ]
    }
   ],
   "source": [
    "shape = x.shape\n",
    "print(\"Shape\", x.shape)\n",
    "\n",
    "size = x.size()\n",
    "print(\"Size\", x.size())\n",
    "\n",
    "dim1, dim2, dim3 = x.size()\n",
    "print(\"Size\", dim1, dim2, dim3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Konvertierung: Tensor ↔ NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensoren können in NumPy-Arrays umgewandelt werden und umgekehrt. Um ein NumPy-Array in einen Tensor zu verwandeln, verwenden wir die Funktion `torch.from_numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy array [[1 2]\n",
      " [3 4]]\n",
      "Tensor tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "np_arr = np.array([[1, 2], [3, 4]])\n",
    "tensor = torch.from_numpy(np_arr)\n",
    "\n",
    "print(\"Numpy array\", np_arr)\n",
    "print(\"Tensor\", tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um einen PyTorch Tensor in ein NumPy-Array zurückzuverwandeln, nutzen wir die `.numpy()`-Methode des Tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor tensor([0, 1, 2, 3])\n",
      "Numpy array [0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.arange(4)\n",
    "np_arr = tensor.numpy()\n",
    "\n",
    "print(\"Tensor\", tensor)\n",
    "print(\"Numpy array\", np_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Umwandlung von Tensoren in NumPy-Arrays setzt voraus, dass sich der Tensor auf der CPU (dem Hauptprozessor) und nicht auf der GPU (Grafikprozessor) befindet. (Mehr zur GPU-Unterstützung später.) Falls dein Tensor auf der GPU ist, musst du zuerst die  `.cpu()`-Methode darauf anwenden. Das sieht dann so aus: `np_arr = tensor.cpu().numpy()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operationen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die meisten Operationen, die in NumPy verfügbar sind, existieren auch in PyTorch. Eine vollständige Liste findest du in der [PyTorch-Dokumentation](https://pytorch.org/docs/stable/tensors.html#). Wir werden die wichtigsten hier besprechen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die grundlegendste Operation - Zwei Tensoren elementweise addieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 tensor([[0.1053, 0.2695, 0.3588],\n",
      "        [0.1994, 0.5472, 0.0062]])\n",
      "X2 tensor([[0.9516, 0.0753, 0.8860],\n",
      "        [0.5832, 0.3376, 0.8090]])\n",
      "Y tensor([[1.0569, 0.3448, 1.2448],\n",
      "        [0.7826, 0.8848, 0.8151]])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.rand(2, 3)\n",
    "x2 = torch.rand(2, 3)\n",
    "y = x1 + x2\n",
    "\n",
    "print(\"X1\", x1)\n",
    "print(\"X2\", x2)\n",
    "print(\"Y\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Aufrufe `x1 + x2` erzeugt einen neuen Tensor, der die Summe der beiden Eingaben enthält. Wir können jedoch auch In-Place-Operationen verwenden, die direkt auf den Speicher eines Tensors wirken. Dadurch verändern wir den Inhalt von `x2` ohne die Möglichkeit, auf Werte vor der Operation zuzugreifen. Ein Beispiel dafür siehst du unten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 (before) tensor([[0.5779, 0.9040, 0.5547],\n",
      "        [0.3423, 0.6343, 0.3644]])\n",
      "X2 (before) tensor([[0.7104, 0.9464, 0.7890],\n",
      "        [0.2814, 0.7886, 0.5895]])\n",
      "X1 (after) tensor([[0.5779, 0.9040, 0.5547],\n",
      "        [0.3423, 0.6343, 0.3644]])\n",
      "X2 (after) tensor([[1.2884, 1.8504, 1.3437],\n",
      "        [0.6237, 1.4230, 0.9539]])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.rand(2, 3)\n",
    "x2 = torch.rand(2, 3)\n",
    "print(\"X1 (before)\", x1)\n",
    "print(\"X2 (before)\", x2)\n",
    "\n",
    "x2.add_(x1)\n",
    "print(\"X1 (after)\", x1)\n",
    "print(\"X2 (after)\", x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In-Place-Operationen werden üblicherweise durch ein angehängtes Unterstrich-Zeichen gekennzeichnet (z.B. \"`add_`\" anstelle von \"`add`\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine weitere häufige Operation ist das Ändern der Form eines Tensors. Ein Tensor der Größe (2,3) kann in jede andere Form mit derselben Anzahl von Elementen umgewandelt werden (z.B. ein Tensor der Größe (6), oder (3,2) usw.). In PyTorch heißt diese Operation `view`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X tensor([0, 1, 2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(6)\n",
    "print(\"X\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "x = x.view(2, 3)\n",
    "print(\"X\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n"
     ]
    }
   ],
   "source": [
    "x = x.permute(1, 0) # Vertauschen der Dimensionen 0 und 1\n",
    "print(\"X\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weitere häufig verwendete Operationen sind Matrixmultiplikationen, die für neuronale Netze unerlässlich sind. Oft haben wir einen Eingabevektor $\\mathbf{x}$, der mithilfe einer gelernten Gewichtsmatrix $\\mathbf{W}$ transformiert wird. Es gibt verschiedene Wege und Funktionen, um Matrixmultiplikationen durchzuführen. Einige davon sind unten aufgeführt:\n",
    "- `torch.matmul`: Führt eine Matrixmultiplikation zweier Tensoren durch. Das spezifische Verhalten hängt von der Dimensionalität ab. Sind beide Tensoren 2D (Matrizen), wird das klassische Matrixprodukt berechnet. Bei höherdimensionalen Tensoren wird Broadcasting unterstützt (Details siehe [Dokumentation](https://pytorch.org/docs/stable/generated/torch.matmul.html?highlight=matmul#torch.matmul)). Kann alternativ mit dem Operator `@` verwendet werden, ähnlich wie in NumPy.\n",
    "- `torch.mm`: Führt eine Matrixmultiplikation zweier Matrizen durch. Broadcasting wird nicht unterstützt (siehe [Dokumentation](https://pytorch.org/docs/stable/generated/torch.mm.html?highlight=torch%20mm#torch.mm)).\n",
    "- `torch.bmm`: Führt eine Matrixmultiplikation unter Berücksichtigung einer Batch-Dimension durch. Hat der erste Tensor $T$ die Form ($b \\times n \\times m$) und der zweite Tensor $R$ die Form ($b \\times m \\times p$), dann hat die Ausgabe $O$ die Form ($b \\times n \\times p$). Dieser Output entsteht durch $b$ separate Matrixmultiplikationen jeweils entsprechender Teilmatrizen von $T$ und $R$: $O_i=T_i @ R_i$\n",
    "- `torch.einsum`: Führt Matrixmultiplikationen und andere Berechnungen (z. B. Summen von Produkten) mithilfe der Einsteinschen Summenkonvention durch. Eine Erklärung der Einsteinschen Summenkonvention findest du in Übung 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalerweise verwenden wir `torch.matmul` oder `torch.bmm`. Im Folgenden können wir eine Matrixmultiplikation mit `torch.matmul` ausprobieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(6)\n",
    "x = x.view(2, 3)\n",
    "print(\"X\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n"
     ]
    }
   ],
   "source": [
    "W = torch.arange(9).view(3, 3) # Wir können mehrere Operationen sequentiell anwenden, indem wir sie in einer einzigen Zeile zusammenfassen.\n",
    "print(\"W\", W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h tensor([[15, 18, 21],\n",
      "        [42, 54, 66]])\n"
     ]
    }
   ],
   "source": [
    "h = torch.matmul(x, W)\n",
    "print(\"h\", h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir müssen häufig einen bestimmten Teil eines Tensors auswählen. Die Indizierung funktioniert genau wie in NumPy. Probieren wir es aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(12).view(3, 4)\n",
    "print(\"X\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 5, 9])\n"
     ]
    }
   ],
   "source": [
    "print(x[:, 1]) # zweite spalte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x[0]) # erste zeile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 7])\n"
     ]
    }
   ],
   "source": [
    "print(x[:2, -1]) # ersten beiden zeilen, davon die letzte spalte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "print(x[1:3, :]) # ersten beiden zeilen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamische Berechnungsgraphen und Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einer der Hauptgründe, PyTorch in Deep-Learning-Projekten einzusetzen, ist die Möglichkeit, **Gradienten (Ableitungen)** von definierten Funktionen automatisch zu berechnen. Da neuronale Netze im Wesentlichen komplexe Funktionen sind, deren Gewichtsmatrizen wir \"lernen\" möchten, benötigen wir diese Funktionalität.  Gewichtsmatrizen, die wir optimieren wollen, werden auch als **Parameter** oder schlicht **Gewichte** bezeichnet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Falls unser neuronales Netz nur einen einzelnen Ausgabewert erzeugen würde, würden wir schlicht von der **Ableitung** sprechen. Da wir jedoch häufig **mehrere** Ausgabevariablen (\"Werte\") haben werden, verwenden wir den Begriff **Gradienten**. Er ist der allgemeinere Begriff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gegeben eine Eingabe $x$, definieren wir unsere Funktion, indem wir diese Eingabe **manipulieren**. Üblicherweise geschieht das durch Matrixmultiplikationen mit Gewichtsmatrizen und Additionen mit sogenannten Bias-Vektoren. Während dieser Manipulationen erstellt PyTorch automatisch einen **Berechnungsgraphen**. Dieser zeigt, wie wir von der Eingabe zur Ausgabe gelangen. PyTorch folgt dem \"**Define-by-Run**\"-Prinzip: Wir führen die Berechnungen einfach aus und PyTorch hält den Graphen für uns fest. So entsteht dynamisch (Schritt für Schritt) ein Berechnungsgraph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zusammengefasst:  Wir müssen lediglich die **Ausgabe** berechnen und können dann PyTorch anweisen, automatisch die **Gradienten** zu bestimmen.\n",
    "\n",
    "**Hinweis: Wozu brauchen wir Gradienten?**  Angenommen, wir haben eine Funktion (ein neuronales Netz) definiert, die für eine Eingabe $\\mathbf{x}$ eine bestimmte Ausgabe $y$ berechnen soll. Dazu definieren wir zusätzlich eine **Fehlerfunktion**, die angibt, wie \"falsch\" unser Netz liegt – wie schlecht es darin ist, $y$ aus $\\mathbf{x}$ vorherzusagen. Mithilfe der Gradienten können wir nun die Gewichte $\\mathbf{W}$, die für die Ausgabe verantwortlich waren, **aktualisieren**. Bei der nächsten Eingabe von $\\mathbf{x}$ wird die Ausgabe dann näher an unserem gewünschten Ergebnis liegen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als erstes müssen wir festlegen, für welche Tensoren Gradienten berechnet werden sollen. Standardmäßig werden Tensoren ohne die Gradienten-Berechnung erstellt. Dies geschieht aus Performance-Gründen, da die Berechnung und Speicherung der Gradienten aufwändig sein kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones((3,))\n",
    "print(x.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können dies für einen bestehenden Tensor ändern, indem wir die Funktion `requires_grad_()` verwenden (der Unterstrich zeigt an, dass es sich um eine In-Place-Operation handelt). Alternativ kannst du bei der Erstellung eines Tensors das Argument `requires_grad=True` an die meisten Initialisierer übergeben, die wir bisher kennengelernt haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x.requires_grad_(True)\n",
    "print(x.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um uns mit dem Konzept des Berechnungsgraphen vertraut zu machen, werden wir einen für die folgende Funktion erstellen:  \n",
    "\n",
    "$$\n",
    "y=\\frac{1}{\\ell(x)} \\sum_i\\left[\\left(x_i+2\\right)^2+3\\right]\n",
    "$$  \n",
    "\n",
    "dabei ist  $\\ell(x)$  die Anzahl der Elemente in $x$. Kurz gesagt berechnen wir einen Mittelwert über den Ausdruck innerhalb der Summe. Nehmen wir an, $x$ enthält Parameter, und wir möchten die Ausgabe $y$ optimieren (entweder maximieren oder minimieren). Dazu müssen wir die Gradienten  $\\partial y / \\partial \\mathbf{x}$  bestimmen. Als Beispiel verwenden wir die Eingabe $\\mathbf{x}=[0,1,2]$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X tensor([0., 1., 2.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(3, dtype=torch.float32, requires_grad=True) # Nur Gleitkomma-Tensoren können Gradienten haben.\n",
    "print(\"X\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lass uns den Berechnungsgraphen nun Schritt für Schritt aufbauen. Mehrere Operationen könnten zwar in einer Zeile kombiniert werden, aber wir werden sie hier einzeln ausführen. Dies hilft uns besser zu verstehen, wie jede Operation zum Berechnungsgraphen hinzugefügt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y tensor(12.6667, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a = x + 2\n",
    "b = a ** 2\n",
    "c = b + 3\n",
    "y = c.mean()\n",
    "\n",
    "print(\"Y\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit den obigen Anweisungen haben wir einen Berechnungsgraphen erstellt, der in etwa so aussieht wie in der folgenden Abbildung:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pytorch_computation_graph.svg\" alt=\"Pytorch Computation Graph\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir berechnen $a$ basierend auf den Eingaben $x$ und der Konstante $2$, danach wird $a$ quadriert und so weiter. Die Visualisierung abstrahiert die Abhängigkeiten zwischen Ein- und Ausgaben der verwendeten Operationen. Jeder Knoten im Berechnungsgraphen hat automatisch eine Funktion zur Berechnung der Gradienten bezüglich seiner Eingaben (`grad_fn`). Dies wurde deutlich, als wir den Ausgabetensor $y$ ausgegeben haben. Deshalb wird der Graph üblicherweise in umgekehrter Richtung dargestellt (Pfeile zeigen vom Ergebnis zu den Eingaben).  Wir können Backpropagation im Graphen durchführen, indem wir die `backward()`-Funktion auf der letzten Ausgabe aufrufen. So werden die Gradienten für jeden Tensor berechnet, der `requires_grad=True` erfüllt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`x.grad` enthält nun den Gradienten  $\\partial y / \\partial x$. Dieser Gradient gibt an, wie eine Änderung in $\\mathbf{x}$ die Ausgabe $\\mathbf{y}$ beeinflusst (bei der aktuellen Eingabe $\\mathbf{x}=[0,1,2]$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.3333, 2.0000, 2.6667])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können die Gradienten auch manuell überprüfen. Dazu werden wir sie mithilfe der Kettenregel berechnen – genau wie PyTorch es getan hat:  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x_i}=\\frac{\\partial y}{\\partial c_i} \\frac{\\partial c_i}{\\partial b_i} \\frac{\\partial b_i}{\\partial a_i} \\frac{\\partial a_i}{\\partial x_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hinweis: Wir haben die Gleichung mit Indexnotation vereinfacht und nutzen dabei, dass  – abgesehen von der Mittelwertbildung  – keine Operation die Elemente des Tensors miteinander kombiniert. Die partiellen Ableitungen sind:  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial a_i}{\\partial x_i}=1, \\quad \\frac{\\partial b_i}{\\partial a_i}=2 \\cdot a_i \\quad \\frac{\\partial c_i}{\\partial b_i}=1 \\quad \\frac{\\partial y}{\\partial c_i}=\\frac{1}{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somit ergeben sich mit der Eingabe $\\mathbf{x}=[0,1,2]$ die Gradienten $\\partial y / \\partial \\mathbf{x}=[4 / 3,2,8 / 3]$. Die vorherige Codezelle sollte das gleiche Ergebnis ausgegeben haben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU-Unterstützung/GPU-Beschleunigung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine entscheidende Stärke von PyTorch ist die Unterstützung von GPUs (Grafikprozessoren). GPUs können viele tausend kleinere Operationen gleichzeitig ausführen. Das macht sie ideal für umfangreiche Matrixoperationen wie sie in neuronalen Netzen vorkommen. Im Vergleich von GPUs und CPUs lassen sich folgende Hauptunterschiede festhalten (Quelle: [Kevin Krewell, 2009](https://blogs.nvidia.com/blog/2009/12/16/whats-the-difference-between-a-cpu-and-a-gpu/)):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Merkmal**              | **CPU**                       | **GPU**                        |\n",
    "| ------------------------ | ----------------------------- | ------------------------------ |\n",
    "| Ausschreibung            | Zentrale Recheneinheit        | Grafikprozessor                |\n",
    "| Anzahl der Kerne         | Einige Kerne                  | Viele Kerne                    |\n",
    "| Latenz                   | Geringe Verzögerung           | Hoher Durchsatz                |\n",
    "| Stärken                  | Gut für serielle Verarbeitung | Gut für parallele Verarbeitung |\n",
    "| Operationen gleichzeitig | Eine Handvoll Operationen     | Tausende Operationen           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPUs und GPUs haben unterschiedliche Vor- und Nachteile. Deshalb sind in vielen Computern beide Komponenten verbaut, um sie für verschiedene Aufgaben zu nutzen. Falls du noch nicht mit GPUs vertraut bist, findest du weitere Informationen in diesem [NVIDIA Blogpost](https://blogs.nvidia.com/blog/2009/12/16/whats-the-difference-between-a-cpu-and-a-gpu/) oder [hier](https://www.intel.com/content/www/us/en/products/docs/processors/what-is-a-gpu.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPUs können das Training deines neuronalen Netzes um den Faktor 100 (oder mehr) beschleunigen. Das ist essentiell für große Netzwerkarchitekturen. PyTorch bietet umfangreiche Funktionen zur Nutzung von GPUs (vor allem von NVIDIA, dank der Bibliotheken [CUDA](https://developer.nvidia.com/cuda-zone) und [cuDNN](https://developer.nvidia.com/cudnn)). Lass uns zunächst prüfen, ob eine GPU verfügbar ist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ist eine GPU verfügbar? False\n",
      "Ist ein MPS (Apple) verfügbar? True\n"
     ]
    }
   ],
   "source": [
    "gpu_avail = torch.cuda.is_available()\n",
    "mps_avail = torch.backends.mps.is_available()\n",
    "print(f\"Ist eine GPU verfügbar? {gpu_avail}\")\n",
    "print(f\"Ist ein MPS (Apple) verfügbar? {mps_avail}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Falls du eine GPU hast, obiger Befehl aber \"`False`\" zurückgibt, stelle sicher, dass die korrekte CUDA-Version installiert ist. Die `d12023`-Umgebung nutzt standardmäßig CUDA 11.8, passend für den Snellius-Supercomputer. Bitte ändere die Version falls nötig (CUDA 11.3 ist derzeit auf Colab verbreitet). Achte bei Google Colab darauf, dass du eine GPU in den Laufzeiteinstellungen ausgewählt hast (im Menü unter `Laufzeit -> Laufzeittyp ändern`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardmäßig werden alle Tensoren, die du erstellst, auf der CPU gespeichert. Um einen Tensor auf die GPU zu übertragen, können wir die Funktion `.to(...)` oder `.cuda()` verwenden. Es empfiehlt sich jedoch, in deinem Code ein Geräteobjekt (\"device object\") zu definieren. Dieses verweist auf die GPU (sofern vorhanden), andernfalls auf die CPU. Wenn du deinen Code anschließend in Bezug auf dieses Geräteobjekt schreibst, kannst du ihn sowohl auf Systemen ohne GPU als auch auf solchen mit GPU bzw. auf Apple Geräten ab M1 Prozessor und neuer ausführen. (Hinweis: Ab PyTorch 1.12 ist die Verwendung von Apple MPS (Metal Performance Shaders) für die GPU-Beschleunigung auf Macs mit Apple Silicon möglich.)Probieren wir es aus. Die Optionen für `torch.device` finden sich in der [Dokumentation](https://pytorch.org/docs/stable/tensor_attributes.html#torch-device). So kannst du das Gerät festlegen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mps\n"
     ]
    }
   ],
   "source": [
    "# Überprüfe, ob Apple MPS verfügbar ist\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    # Falls MPS nicht verfügbar ist, verwende CUDA wenn verfügbar, sonst CPU\n",
    "    torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    \n",
    "print(\"Device\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lass uns nun einen Tensor erstellen und ihn auf das zuvor festgelegte Gerät übertragen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(2, 3)\n",
    "x = x.to(device)\n",
    "\n",
    "print(\"X\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Falls du eine GPU hast, solltest du nun das Attribut `device='cuda:0'` oder `device='mps:0'` neben deinem Tensor sehen. Die Null hinter cuda/mps  gibt an, dass dies die erste GPU in deinem System ist. PyTorch unterstützt auch Systeme mit mehreren GPUs. Dies wirst du jedoch erst für sehr große Netzwerke benötigen (bei Interesse findest du mehr in der [PyTorch-Dokumentation](https://pytorch.org/docs/stable/distributed.html#distributed-basics)). Wir können auch die Laufzeit einer großen Matrixmultiplikation auf der CPU und der GPU vergleichen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laufzeit auf der CPU: 0.32534s\n",
      "Laufzeit auf MPS: 0.21464s\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5000, 5000)\n",
    "\n",
    "## CPU Version\n",
    "start_time = time.time()\n",
    "_ = torch.matmul(x, x)\n",
    "end_time = time.time()\n",
    "print(f\"Laufzeit auf der CPU: {(end_time - start_time):6.5f}s\")\n",
    "\n",
    "## GPU Version\n",
    "if device.type == \"mps\":\n",
    "    x = x.to(device)\n",
    "    _ = torch.matmul(x, x)\n",
    "    # MPS arbeitet asynchron. Daher müssen wir andere Funktionen zur Zeitmessung verwenden.\n",
    "    start = torch.mps.Event(enable_timing=True)\n",
    "    end = torch.mps.Event(enable_timing=True)\n",
    "    start.record()\n",
    "    _ = torch.matmul(x, x)\n",
    "    end.record()\n",
    "    torch.mps.synchronize()\n",
    "    print(f\"Laufzeit auf MPS: {0.001 * start.elapsed_time(end):6.5f}s\")\n",
    "elif device.type == \"cuda\":\n",
    "    x = x.to(device)\n",
    "    _ = torch.matmul(x, x)\n",
    "    # CUDA arbeitet auch asynchron. Daher müssen wir andere Funktionen zur Zeitmessung verwenden.\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "    _ = torch.matmul(x, x)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    print(f\"Laufzeit auf CUDA: {0.001 * start.elapsed_time(end):6.5f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abhängig von der Größe der Operation sowie deiner CPU/GPU, kann die Beschleunigung um mehr als das 50-fache betragen. Da Matrixmultiplikationen (`matmul`) in neuronalen Netzen sehr häufig sind, lässt sich der enorme Vorteil des GPU-basierten Trainings leicht erkennen. Die Laufzeitschätzung kann hier etwas ungenau sein, da wir sie nicht mehrfach durchgeführt haben. Du kannst das gerne erweitern, allerdings verlängert das dann auch die Ausführungszeit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei der Generierung von Zufallszahlen sind die Startwerte (\"Seeds\") zwischen CPU und GPU nicht synchronisiert. Um reproduzierbare Ergebnisse zu gewährleisten, müssen wir den Seed daher separat auf der GPU setzen. Beachte, dass verschiedene GPU-Architekturen selbst bei identischem Code nicht zwangsläufig die gleichen Zufallszahlen erzeugen. Trotzdem möchten wir natürlich vermeiden, dass unser Code bei jeder Ausführung auf derselben Hardware unterschiedliche Ausgaben liefert. Deshalb setzen wir den Seed auch auf der GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auch für GPU-Operationen muss ein separater Startwert (Seed) gesetzt werden.\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "elif torch.backends.mps.is_available():\n",
    "    torch.mps.manual_seed(42)\n",
    "    \n",
    "# Zusätzlich sind manche Operationen auf der GPU aus Effizienzgründen stochastisch implementiert.\n",
    "# Dies kann dazu führen, dass derselbe Code bei mehrfacher Ausführung leicht unterschiedliche Ergebnisse liefern kann.\n",
    "# Stochastisch: Bedeutet in diesem Kontext, dass die Operation ein zufälliges Element enthält. Die Ausgabe ist also bei jedem Durchlauf nicht exakt gleich.\n",
    "# Effizienz: Durch die stochastische Implementierung können bestimmte Operationen auf der GPU deutlich schneller ausgeführt werden, auch wenn das Ergebnis minimal variieren kann.\n",
    "# Aus Gründen der Reproduzierbarkeit möchten wir sicherstellen, dass alle Operationen auf der GPU (falls verwendet) deterministisch ausgeführt werden.\n",
    "if device.type == \"cuda\":\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lernen durch Beispiele: Kontinuierliches XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn wir ein neuronales Netz in PyTorch implementieren möchten, könnten wir alle Parameter (Gewichtsmatrizen, Bias-Vektoren) als Tensoren definieren (`requires_grad=True`) und PyTorch die Gradienten berechnen lassen. Bei vielen Parametern wird das jedoch schnell unübersichtlich. Deshalb gibt es in PyTorch das Paket `torch.nn`, das die Erstellung neuronaler Netze deutlich vereinfacht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir werden die Bibliotheken und alle zusätzlichen Komponenten vorstellen, die du für das Training eines neuronalen Netzes in PyTorch benötigst. Dazu nutzen wir einen einfachen Klassifikator an einem bekannten Beispiel: XOR. Gegeben seien zwei binäre Eingaben $x_1$ und $x_2$. Das vorherzusagende Label ist 1, wenn entweder $x_1$ oder $x_2$ gleich 1 ist (und das jeweils andere 0 ist), andernfalls ist das Label 0. Dieses Beispiel wurde berühmt, weil ein einzelnes Neuron (d.h. ein linearer Klassifikator) diese simple Funktion nicht lernen kann. Daher werden wir ein kleines neuronales Netz erstellen, das diese Aufgabe bewältigen kann. Um es etwas interessanter zu machen, verschieben wir das XOR-Problem in einen kontinuierlichen Raum und fügen den binären Eingaben etwas Gaußsches Rauschen hinzu. Eine mögliche Trennung in einem XOR-Datensatz könnte dann so aussehen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/continuous_xor.svg\" alt=\"Continious XOR\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Das Modell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Paket `torch.nn` bietet eine Reihe nützlicher Klassen, wie lineare Netzwerk-Layer, Aktivierungsfunktionen, Verlustfunktionen usw. Eine vollständige Liste findest du [hier](https://pytorch.org/docs/stable/nn.html). Falls du einen bestimmten Netzwerk-Layer benötigst, lohnt es sich, zuerst in der Dokumentation des Pakets nachzusehen. Es ist sehr wahrscheinlich, dass `torch.nn` die Implementierung bereits enthält, sodass du dir das Schreiben eigener Layer sparen kannst. Nachfolgend importieren wir das Paket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zusätzlich zu `torch.nn` gibt es noch `torch.nn.functional`. Dieses Paket enthält Funktionen, die in Netzwerk-Layern verwendet werden. Im Gegensatz dazu definiert `torch.nn` diese Funktionen als `nn.Modules` (mehr dazu weiter unten). `torch.nn` selbst nutzt auch viele Funktionalitäten aus `torch.nn.functional`. Das functional-Paket ist daher in vielen Situationen praktisch – wir importieren es hier ebenfalls.\n",
    "\n",
    "Module: Kapseln Komponenten eines neuronalen Netzes, während das functional-Paket lose Funktionen bereitstellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch wird ein neuronales Netz aus Modulen aufgebaut. Module können wiederum andere Module enthalten. Das gesamte neuronale Netz wird ebenfalls als Modul betrachtet. Die grundlegende Struktur eines Moduls sieht so aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self): # hier werden alle attribute initialisiert die für das Modul spezifisch sind\n",
    "        super().__init__() # sorgt dafür, dass ale notwendigen Initialisierungen der Basisklasse nn.Module sichergestellt sind\n",
    "        # hier könnten jetzt initialisierungen für das modul stehen\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # hier sind die funktionen enthalten, die die berechnungen in dem modul durchführen\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die `forward`-Funktion definiert die Berechnungen, die innerhalb des Moduls stattfinden. Sie wird ausgeführt, wenn du das Modul aufrufst (z.B. `nn = MyModule(); nn(x)`). In der `__init__`-Funktion erzeugen wir üblicherweise die Parameter des Moduls, indem wir `nn.Parameter` verwenden oder andere Module definieren, die dann in der forward-Funktion zum Einsatz kommen. Die Berechnung der Gradienten (backward) erfolgt automatisch, könnte bei Bedarf aber auch überschrieben werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nutzen wir nun die vordefinierten Module des `torch.nn`-Pakets, um unser eigenes kleines neuronales Netz zu entwerfen. Wir erstellen ein minimales Netzwerk mit einem Input-Layer, einem versteckten Layer (hidden layer) mit tanh als Aktivierungsfunktion und einem Output-Layer. Unser Netzwerk könnte daher in etwa so aussehen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/small_neural_network.svg\" alt=\"Small neural network\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Eingabeneuronen (blau) repräsentieren die Koordinaten $x1$ und $x2$ eines Datenpunktes. Die Neuronen des versteckten Layers (hidden layer) einschließlich der tanh-Aktivierungsfunktion werden in weiß dargestellt, das Ausgabeneuron in rot. In PyTorch können wir dies folgendermaßen definieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs):\n",
    "        super().__init__()\n",
    "        # Erstelle Instanzen der Module, die wir für den Aufbau unseres neuronalen Netzes benötigen.\n",
    "        # Dies umfasst die Definition der Schichten, Aktivierungsfunktionen usw.\n",
    "        self.linear1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.act_fn = nn.Tanh()\n",
    "        self.linear2 = nn.Linear(num_hidden, num_outputs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Lasse die Eingabe durch das Modell laufen, um dessen Ausgabe (Vorhersage) zu erhalten.\n",
    "        x = self.linear1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die Beispiele in diesem Notebook verwenden wir ein kleines neuronales Netz mit zwei Eingabeneuronen und vier Neuronen im versteckten Layer (hidden layer). Da wir eine binäre Klassifikation durchführen, nutzen wir ein einzelnes Ausgabeneuron. Beachte, dass wir noch keine Sigmoid-Funktion auf die Ausgabe anwenden. Dies liegt daran, dass andere Funktionen, insbesondere die Verlustfunktion, effizienter und genauer auf den ursprünglichen Ausgaben (statt den Ausgaben nach der Sigmoid-Funktion) berechnet werden können. Die genauen Gründe dafür besprechen wir später."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleClassifier(\n",
      "  (linear1): Linear(in_features=2, out_features=4, bias=True)\n",
      "  (act_fn): Tanh()\n",
      "  (linear2): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = SimpleClassifier(num_inputs=2, num_hidden=4, num_outputs=1)\n",
    "# mit print können wir uns alle sub-modules anzeigen lassen\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Ausgeben des Modells auf der Konsole listet alle enthaltenen Untermodule auf. Die Parameter eines Moduls lassen sich mithilfe der Funktionen `parameters()` oder `named_parameters()` abrufen. Letztere versieht zudem jedes Parameterobjekt mit einem Namen. In unserem kleinen neuronalen Netz haben wir die folgenden Parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter linear1.weight, shape torch.Size([4, 2])\n",
      "Parameter linear1.bias, shape torch.Size([4])\n",
      "Parameter linear2.weight, shape torch.Size([1, 4])\n",
      "Parameter linear2.bias, shape torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter {name}, shape {param.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jedes lineare Layer besitzt eine Gewichtsmatrix mit der Form `[output, input]` sowie einen Bias-Vektor der Form `[output]`. Die Aktivierungsfunktion `tanh` benötigt keine Parameter. Beachte, dass Parameter nur für `nn.Module`-Objekte registriert werden, die direkte Objektattribute sind (also z.B. `self.a = ...`). Wenn du Module innerhalb einer Liste definierst, werden ihre Parameter nicht für das übergeordnete Modul registriert. Dies kann Probleme bei der Optimierung deines Moduls verursachen. Es gibt Alternativen wie `nn.ModuleList`, `nn.ModuleDict` und `nn.Sequential`, die es dir erlauben, verschiedene Datenstrukturen für Module zu verwenden. Wir werden diese Möglichkeiten in späteren Tutorials verwenden und dort näher erläutern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Die Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusätzliche Features, die wir noch nicht besprochen haben"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uvadlc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
